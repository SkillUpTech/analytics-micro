# Containerized OpenEdX Analyticstack
## About
This project is a work-in-progress implementation of the complete OpenEdX Analyticstack as containerized-microservices in Docker.

Please note this is *NOT* an official EdX repository. Skill-Up Technologies (Flexible Road, LLC) is not affiliated in any way with EdX, nor is this repository or the software therein endorsed by EdX.

### Services

This implementation of the OpenEdX Analyticstack is broken into eight images. The source files for each image can be found in the `services/` directory. The services are divided into three categories ("stacks"), which are reflected in the directory structure of the repository: the Hadoop stack (`hadoop`), the OpenEdX stack (`openedx`), and the web stack (`web`).

By default, all images are tagged with the registry `sktpub.azurecr.io/analytics-micro`.

#### Hadoop

The Hadoop stack is composed of images for the following servers:

- Apache Hadoop Distributed File System (`hadoop/hdfs`)
- Apache Hive Server (`hadoop/hive-server`)
- Apache Spark (`hadoop/spark`)
- The OpenEdX Analytics Pipeline (`hadoop/pipeline`)

At run time, these images are used to create containers of the same name (minus the `hadoop/` prefix), with one exception--the `hdfs` image is actually used to generate two containers: the HDFS NameNode (`namenode`) and DataNode (`datanode`).

All of the above images share a common base called `hadoop/base`.

#### OpenEdX

The OpenEdX stack is composed of images for the following servers:

- The OpenEdX Analytics Data API (`openedx/api`)
- The OpenEdX Analytics Dashboard (`openedx/insights`)

These images are used to generate containers named `api` and `insights`, respectively. 

Both use a base image called `openedx/base`.

#### Web

The web stack is composed of images for the following servers:

- MySQL (`web/mysql`)
- NginX (`web/nginx`)
- Elasticsearch\*

These images are used to generate containers called `mysql`, `nginx`, and `elasticsearch`, respectively.

\*The Elasticsearch image is pulled directly from Docker Hub and is not customized by this project. As such, it lacks its own directory and Dockerfile.
## Development

### Setup

To begin, clone this repository and enter the project directory:

```shell
git clone git@github.com:SkillUpTech/analytics-micro.git
cd analytics-micro
```

### Initialization

#### Generating Targets

This project uses a Makefile to facilitate building Docker images for each service. 
In order to do so, a file named `.target` is placed alongside the `Dockerfile` for each image. 
The `.target` file is used internally by `Make` to track the time at which each image was last built.
The process of generating these files is performed automatically when you execute the following command:

```shell
make init
```

#### Managing Configuration Values

In addition to generating `.target` files, this command also generates templates for each user-configuration file ("config file") required by Analytics-Micro stack.
These files can be found in the `conf/` directory. 
Don't worry: if you've already placed configuration values in these files, they won't be overwritten by this command.

Once you have initialized the project using `make init`, you'll need to review the values in the following files:

- `conf/analytics.env`
    - Composed primarily of variables which will be substituted into `insights.yml` and `analytics_api.yml`.
- `conf/mysql.env`
    - Defines the password for the root user generated by the `mysql` container upon its first run.
- `conf/pipeline.env`
    - Defines a few extra values required only by the `pipeline` container.

Internally, the `.env` files define environment variables which are passed to the containers at run-time. These environment variables are used for a variety of purposes, depending on the services. Most commonly, they are used to render templates into settings files when the container first boots.

### Building the Images

The Makefile ensures that each image is built in the correct order. This ensures that, for instance, a child-image (such as `openedx/insights`) is never built before its parent (such as `openedx/base`).

To build all images for all stacks in sequence, type:

```shell
make build
```

For your convenience, the command `make dev` has the effect of running `make init` then `make build`.

The process of building all images can be somewhat lengthy. However, it only needs to be done once. From there, only portions of the images which you modify will need to be rebuilt in the future.

### Bringing Up the OpenEdX & Web Stacks

During typical operation, only the OpenEdX and Web stacks will be active. In contrast, the Hadoop stack is designed to be brought online once per day, then destroyed immediately thereafter (more on that in the next section).

To bring the OpenEdX and Web stacks online, execute the following command:

```shell
make up
```

#### Cold-Start Behavior

Executing `make up` for the first time (or after deleting the Docker volumes) triggers a chain-reaction of initialization tasks automatically:

- The `mysql` container will immediately create all databases and users required to run the stack.
    - Until this process is finished, the `insights` and `api` containers will be unable to complete their initialization. Depending on how long it takes, they may error out as a result. Not to worry, though--they'll simply restart themselves and keep trying again tirelessly until the database is finally initialized. The error messages are safe to ignore, in this case.
- Then, the `insights` and `api` containers will apply all database migrations needed by the EdX Analytics Dashboard and EdX Analytics Data API Django apps, respectively.
- Once `insights` and `api` come online, `nginx` will begin proxying web traffic accordingly.

### The Analytics Pipeline

#### A Brief Overview of the Analytics Pipeline

This document assumes basic familiarity with the OpenEdX Analytics Pipeline's purpose and expected outcomes. However, as a small refresher, the structure is also summarized below.

The pipeline's overarching purpose is to consume raw data in the form of tracking-logs generated by the OpenEdX platform (the LMS and Studio, for instance) and produce summarized, aggregate data to give teachers/staff a high-level overview of students' interactions with their courses.

To do so, it first loads the tracking logs into an HDFS persistent storage volume. Then, it initiates a series of MapReduce tasks (executed by Spark in this implementation) which compute aggregate statistics based on the log entires. These statistics are output into Hive (an SQL-like interface to HDFS) tables, which are in turn transferred into a MySQL database ('reports') for consumption by the OpenEdx Analytics Data API and Analytics Dashboard.

#### Running the Insights-Update Hadoop Tasks

In order to populate analytics data from the tracking logs, you'll need to configure the hadoop stack to be run daily. This can be done, for instance, using a cron-job. 

Begin by populating all relevant tracking logs into the pipeline's data volume (by default: `data/hadoop/`). This can be accomplished using `rsync`, for instance.

To bring the Hadoop stack online, execute the pipeline tasks, then bring the stack back down again, use the following command:

```shell
make hadoop-tasks
```

This command should be run while the core (OpenEdx and web) stack is also running concurrently.
