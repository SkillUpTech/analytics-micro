# Containerized OpenEdX Analyticstack
## About
This project is a work-in-progress implementation of the complete OpenEdX Analyticstack as containerized-microservices in Docker.

Please note this is *NOT* an official EdX repository. Skill-Up Technologies (Flexible Road, LLC) is not affiliated in any way with EdX, nor is this repository or the software therein endorsed by EdX.

### Services

This implementation of the OpenEdX Analyticstack is broken into eight images. The source files for each image can be found in the `services/` directory. The services are divided into three categories ("stacks"), which are reflected in the directory structure of the repository: the Hadoop stack (`hadoop`), the OpenEdX stack (`openedx`), and the web stack (`web`).

By default, all images are tagged with the registry `sktpub.azurecr.io/analytics-micro`.

#### Hadoop

The Hadoop stack is composed of images for the following servers:

- Apache Hadoop Distributed File System (`hadoop/hdfs`)
- Apache Hive Server (`hadoop/hive-server`)
- Apache Spark (`hadoop/spark`)
- The OpenEdX Analytics Pipeline (`hadoop/pipeline`)

At run time, these images are used to create containers of the same name (minus the `hadoop/` prefix), with one exception--the `hdfs` image is actually used to generate two containers: the HDFS NameNode (`namenode`) and DataNode (`datanode`).

All of the above images share a common base called `hadoop/base`.

#### OpenEdX

The OpenEdX stack is composed of images for the following servers:

- The OpenEdX Analytics Data API (`openedx/api`)
- The OpenEdX Analytics Dashboard (`openedx/insights`)

These images are used to generate containers named `api` and `insights`, respectively. 

Both use a base image called `openedx/base`.

#### Web

The web stack is composed of images for the following servers:

- MySQL (`web/mysql`)
- NginX (`web/nginx`)
- Elasticsearch\*

These images are used to generate containers called `mysql`, `nginx`, and `elasticsearch`, respectively.

\*The Elasticsearch image is pulled directly from Docker Hub and is not customized by this project. As such, it lacks its own directory and Dockerfile.
## Development

### Setup

To begin, clone this repository and enter the project directory:

```shell
git clone git@github.com:SkillUpTech/analytics-micro.git
cd analytics-micro
```

### Initialization

#### Generating Targets

This project uses a Makefile to facilitate building Docker images for each service. 
In order to do so, a file named `.target` is placed alongside the `Dockerfile` for each image. 
The `.target` file is used internally by `Make` to track the time at which each image was last built.
The process of generating these files is performed automatically when you execute the following command:

```shell
make init
```

#### Managing Configuration Values

In addition to generating `.target` files, this command also generates templates for each user-configuration file ("config file") required by Analytics-Micro stack.
These files can be found in the `conf/` directory. 
Don't worry: if you've already placed configuration values in these files, they won't be overwritten by this command.

Once you have initialized the project using `make init`, you'll need to review the values in the following files:

- `conf/analytics.env`
    - Composed primarily of variables which will be substituted into `insights.yml` and `analytics_api.yml`.
- `conf/mysql.env`
    - Defines the password for the root user generated by the `mysql` container upon its first run.
- `conf/pipeline.env`
    - Defines a few extra values required only by the `pipeline` container.

Internally, the `.env` files define environment variables which are passed to the containers at run-time. These environment variables are used for a variety of purposes, depending on the services. Most commonly, they are used to render templates into settings files when the container first boots.

### Building the Images

The Makefile ensures that each image is built in the correct order. This ensures that, for instance, a child-image (such as `openedx/insights`) is never built before its parent (such as `openedx/base`).

To build all images for all stacks in sequence, type:

```shell
make build
```

For your convenience, the command `make dev` has the effect of running `make init` then `make build`.

The process of building all images can be somewhat lengthy. However, it only needs to be done once. From there, only portions of the images which you modify will need to be rebuilt in the future.

### Bringing Up the OpenEdX & Web Stacks

During typical operation, only the OpenEdX and Web stacks will be active. In contrast, the Hadoop stack is designed to be brought online once per day, then destroyed immediately thereafter (more on that in the next section).

To bring the OpenEdX and Web stacks online, execute the following command:

```shell
make up
```

#### Cold-Start Behavior

Executing `make up` for the first time (or after deleting the Docker volumes) triggers a chain-reaction of initialization tasks automatically:

- The `mysql` container will immediately create all databases and users required to run the stack.
    - Until this process is finished, the `insights` and `api` containers will be unable to complete their initialization. Depending on how long it takes, they may error out as a result. Not to worry, though--they'll simply restart themselves and keep trying again tirelessly until the database is finally initialized. The error messages are safe to ignore, in this case.
- Then, the `insights` and `api` containers will apply all database migrations needed by the EdX Analytics Dashboard and EdX Analytics Data API Django apps, respectively.
- Once `insights` and `api` come online, `nginx` will begin proxying web traffic accordingly.

### The Analytics Pipeline

#### A Brief Overview of the Analytics Pipeline

This document assumes basic familiarity with the OpenEdX Analytics Pipeline's purpose and expected outcomes. However, as a small refresher, the structure is also summarized below.

The pipeline's overarching purpose is to consume raw data in the form of tracking-logs generated by the OpenEdX platform (the LMS and Studio, for instance) and produce summarized, aggregate data to give teachers/staff a high-level overview of students' interactions with their courses.

To do so, it first loads the tracking logs into an HDFS persistent storage volume. Then, it initiates a series of MapReduce tasks (executed by Spark in this implementation) which compute aggregate statistics based on the log entires. These statistics are output into Hive (an SQL-like interface to HDFS) tables, which are in turn transferred into a MySQL database ('reports') for consumption by the OpenEdx Analytics Data API and Analytics Dashboard.

#### Running the Insights-Update Hadoop Tasks

In order to populate analytics data from the tracking logs, you'll need to configure the hadoop stack to be run daily. This can be done, for instance, using a cron-job. 

Begin by populating all relevant tracking logs into the pipeline's data volume (by default: `data/tracking-logs/`). This can be accomplished using `rsync`, for instance.

To bring the Hadoop stack online, execute the pipeline tasks, then bring the stack back down again, use the following command:

```shell
make hadoop-tasks
```

This command should be run while the core (OpenEdx and web) stack is also running concurrently.


### Pushing Images to the Registry

By default, all images are tagged with the `sktpub.azurecr.io/analytics-micro` container registry. To push to a registry, ensure you have access. The following directions assume you are using an Azure Container Registry (acr) and will use the aforementioned registry as an example.

#### Authenticate with the `az` CLI

```shell
az login --use-device-code
```

Follow the prompt to authenticate with Azure.

#### Authenticate with the Registry

```shell
az acr login --name sktpub
```

#### Push the Images

To push all tags of all built images to the registry, execute:

```shell
make push
```

### Makefile

#### About

*This section delves into the technical details of the Makefile used by this repository. This knowledge is generally not important unless you wish to modify the Makefile itself. Feel free to skip to the next section for a description of the basic `make` commands.*

Most development-related tasks are fully automated in the `Makefile` at the root of this repository.

The primary reason for using Make is to ensure that images are always built in the correct order. As previously discussed, an empty file named `.target` is placed alongside each `Dockerfile` in the `images/` directory. Each time an image is built using `make build`, the timestamp of the corresponding `.target` file is updated. This allows Make to determine whether the `Dockerfile` (or any other file/directory) alongside each `.target` file has been modified since the last build. If it has, the corresponding image will be rebuilt upon the next invocation of `make build`. If not, the build for that image can be skipped.

As such, the syntax for building the image for a single service is as follows:

```shell
make services/IMAGE_PATH/.target
```

`IMAGE_PATH` is the path to an image's context directory relative to `services/`; for example, `openedx/api` for the Analytics Data API image, and `hadoop/spark` for the Apache Spark image. So, for example, to build *only* the `hive-server` image, you would type:

```shell
make services/hadoop/hive-server/.target
```

For the sake of convenience, a rule is defined to allow a shorthand sytax for building individual images. The equivalent shorthand for the above command is:

```shell
make build.hadoop.hive-server
```

#### Command Reference

When in doubt, to view a list of all available commands, simply execute `make` or:

```shell
make help
```

Below are example invocations of `Makefile` command. Where applicable, we use the `insights` service (or its image) as an example. All such commands also work for any other service. For example, anywhere you see `insights` (or `openedx.insights`), you could substitute `pipeline` (or `hadoop.pipeline`) in order to work with the `pipeline` service instead of the `insights` service.

##### Build Commands

Build any image(s) which have been modified since the last build (or which have not yet been built at all):

```shell
make build
```

Build a single image using its qualified name (the path to the image context relative to `services/` but with slashes replaced with dots):

```shell
make build.openedx.insights
```

Discard and recreate the `.target` files, then rebuild all images:

```shell
make force-rebuild
```

##### Initialization Commands

Create all target files and initialize configuration files in the `conf/` directory:

```shell
make init
```

Format the HDFS NameNode prior to its first run (or after recreating its volume):

```shell
make format-namenode
```

##### Start Commands

Bring up the core (openedx and web) stacks, then tail their log streams:

```shell
make up
```

Create the Hadoop services and run the update tasks:

```shell
make hadoop-tasks
```

Launch a single service along with its explicit dependencies (as determined by the `depends_on` field in `docker-compose.yml`) without bringing up the rest of the stack:

```shell
make start.insights
```

##### Stop Commands

Stop and remove all running containers and networks:

```shell
make down
```

Stop and remove all running containers, networks, ⚠️AND VOLUMES⚠️ (this will delete ALL data, including HDFS, MySQL, and Elasticsearch data):

⚠️⚠️⚠️
```shell
make destroy
```
⚠️⚠️⚠️

##### Debugging Commands

Attach an interactive shell session to the container of a running service:

```shell
make exec.insights
```

Create and attach to a new instance of a container for a service (does not need to be running). This is useful for debugging without needing to actually start the stack, or for debugging a failing service whose container is unstable (frequently erroring out and restarting):

```shell
make shell.insights
```

##### Misc Commands

Follow the log stream(s) of all running services:

```shell
make logs
```


